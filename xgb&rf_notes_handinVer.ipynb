{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb with **notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "RANDOM_SEED = 3612\n",
    "\n",
    "# Step 1: Load and inspect the data\n",
    "print(\"Loading notes data...\")\n",
    "notes_df = pd.read_csv('notes.csv')\n",
    "\n",
    "# Step 2: Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "notes_df['processed_text'] = notes_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Step 3: Extract TF-IDF features\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 3))  # Expand the n-gram range\n",
    "text_features = tfidf.fit_transform(notes_df['processed_text']).toarray()\n",
    "\n",
    "# Save the extracted features\n",
    "text_features_df = pd.DataFrame(text_features, columns=tfidf.get_feature_names_out())\n",
    "text_features_df['id'] = notes_df['id']\n",
    "text_features_df.to_csv('notes_features_tfidf.csv', index=False)\n",
    "\n",
    "# Step 4: Load training and testing data\n",
    "print(\"Loading train and test data...\")\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Merge the text features with train and test data\n",
    "train_data = train_data.merge(text_features_df, on='id', how='left')\n",
    "test_data = test_data.merge(text_features_df, on='id', how='left')\n",
    "\n",
    "# Fill missing values with zero\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "# Step 5: Prepare features and labels\n",
    "feature_cols = [col for col in train_data.columns if col not in ['id', 'readmitted_within_30days', 'text', 'processed_text']]\n",
    "\n",
    "# Check if all feature columns are numeric\n",
    "non_numeric_cols = [col for col in feature_cols if train_data[col].dtype == 'object']\n",
    "if non_numeric_cols:\n",
    "    print(f\"Non-numeric columns detected and excluded: {non_numeric_cols}\")\n",
    "    feature_cols = [col for col in feature_cols if col not in non_numeric_cols]\n",
    "\n",
    "# Prepare training and testing data\n",
    "X = train_data[feature_cols].values\n",
    "y = train_data['readmitted_within_30days'].values\n",
    "X_test = test_data[feature_cols].values\n",
    "\n",
    "\n",
    "# Step 6: Cross-validation\n",
    "print(\"Starting cross-validation...\")\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "auc_scores = []\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 3,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lambda\": 1.0,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"seed\": RANDOM_SEED,\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"--- Fold {fold + 1} ---\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(dval)\n",
    "    test_preds_fold = model.predict(dtest)\n",
    "    oof_preds.extend(val_preds)\n",
    "    test_preds.append(test_preds_fold)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, val_preds)\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Fold {fold + 1} Validation AUC: {auc:.4f}\")\n",
    "\n",
    "# Step 7: Average predictions for the test set\n",
    "final_test_preds = sum(test_preds) / len(test_preds)\n",
    "\n",
    "# Step 8: Save the prediction results\n",
    "test_data['readmitted_within_30days_pred'] = final_test_preds\n",
    "averaged_predictions = (\n",
    "    test_data.groupby('id', as_index=False)['readmitted_within_30days_pred']\n",
    "    .mean()\n",
    ")\n",
    "averaged_predictions.to_csv('test_predictions_tfidf_cv.csv', index=False)\n",
    "\n",
    "print(f\"Mean Validation AUC: {sum(auc_scores) / len(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(\"Averaged predictions saved to 'test_predictions_tfidf_cv.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb with notes and MI method of feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import DMatrix, train\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 参数设置\n",
    "RANDOM_SEED = 3612\n",
    "\n",
    "def main():\n",
    "    # 加载 notes 数据并提取 TF-IDF 特征\n",
    "    print(\"Loading notes data and extracting TF-IDF features...\")\n",
    "    text_features_df = pd.read_csv('notes_features_tfidf.csv')\n",
    "    tfidf_cols = [col for col in text_features_df.columns if col != 'id']\n",
    "\n",
    "    # 加载 train 和 test 数据\n",
    "    print(\"Loading train and test data...\")\n",
    "    train_data = pd.read_csv(\"train.csv\")\n",
    "    test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    # 合并 TF-IDF 特征\n",
    "    print(\"Merging TF-IDF features with train and test data...\")\n",
    "    train_data = train_data.merge(text_features_df, on='id', how='left').fillna(0)\n",
    "    test_data = test_data.merge(text_features_df, on='id', how='left').fillna(0)\n",
    "\n",
    "    # 准备 TF-IDF 特征矩阵\n",
    "    X = train_data[tfidf_cols].values\n",
    "    y = train_data['readmitted_within_30days'].values\n",
    "    X_test = test_data[tfidf_cols].values\n",
    "\n",
    "    # 检查特征是否是数值型\n",
    "    print(\"Validating feature matrix...\")\n",
    "    if not np.issubdtype(X.dtype, np.number):\n",
    "        raise ValueError(\"Non-numeric data detected in features. Please check the preprocessing step.\")\n",
    "\n",
    "    # 1. 使用 Mutual Information 筛选特征\n",
    "    print(\"Selecting features using Mutual Information...\")\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=RANDOM_SEED)\n",
    "    feature_importance = pd.DataFrame({'feature': tfidf_cols, 'mi_score': mi_scores})\n",
    "    selected_features = feature_importance[feature_importance['mi_score'] > 0.08]['feature'].tolist()  # 阈值可调整\n",
    "    print(f\"Number of selected features: {len(selected_features)}\")\n",
    "\n",
    "    if len(selected_features) == 0:\n",
    "        raise ValueError(\"No features selected after Mutual Information filtering. Adjust the threshold or preprocessing steps.\")\n",
    "\n",
    "    # 更新特征矩阵\n",
    "    X = train_data[selected_features].values\n",
    "    X_test = test_data[selected_features].values\n",
    "\n",
    "    # 交叉验证\n",
    "    print(\"Starting cross-validation...\")\n",
    "    kf = KFold(n_splits=6, shuffle=True, random_state=RANDOM_SEED)\n",
    "    test_predictions = np.zeros(X_test.shape[0])\n",
    "    fold_aucs = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"--- Fold {fold + 1} ---\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # 转换为 DMatrix 格式\n",
    "        dtrain = DMatrix(X_train, label=y_train)\n",
    "        dval = DMatrix(X_val, label=y_val)\n",
    "        dtest = DMatrix(X_test)\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"learning_rate\": 0.007,\n",
    "            \"max_depth\": 4,\n",
    "            \"min_child_weight\": 3,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"alpha\": 0.1,\n",
    "            \"lambda\": 1,\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"seed\": RANDOM_SEED,\n",
    "        }\n",
    "\n",
    "        # 训练模型\n",
    "        model = train(params, dtrain, num_boost_round=2000, evals=[(dval, \"validation\")],\n",
    "                      early_stopping_rounds=50, verbose_eval=50)\n",
    "\n",
    "        # 验证集AUC\n",
    "        val_pred = model.predict(dval)\n",
    "        val_auc = roc_auc_score(y_val, val_pred)\n",
    "        fold_aucs.append(val_auc)\n",
    "        print(f\"Fold {fold + 1} Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # 累加测试集预测\n",
    "        test_predictions += model.predict(dtest) / kf.n_splits\n",
    "\n",
    "    # 打印交叉验证结果\n",
    "    print(f\"Mean Validation AUC: {np.mean(fold_aucs):.4f} ± {np.std(fold_aucs):.4f}\")\n",
    "\n",
    "    # 保存测试集预测结果\n",
    "    test_data['readmitted_within_30days_pred'] = test_predictions\n",
    "    averaged_predictions = test_data[['id', 'readmitted_within_30days_pred']].groupby('id').mean()\n",
    "    averaged_predictions.to_csv('test_predictions_optimized.csv')\n",
    "    print(\"Averaged predictions saved to 'test_predictions_optimized.csv'.\")\n",
    "\n",
    "    # 可视化特征重要性\n",
    "    feature_importance['mi_score'] = mi_scores\n",
    "    feature_importance = feature_importance.sort_values(by='mi_score', ascending=False).head(20)\n",
    "    plt.barh(feature_importance['feature'], feature_importance['mi_score'])\n",
    "    plt.xlabel('Mutual Information Score')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest with notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jeff/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jeff/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading notes data...\n",
      "Extracting TF-IDF features...\n",
      "Loading train and test data...\n",
      "Non-numeric columns detected and excluded: ['dicom_id', 'ViewPosition', 'image_path']\n",
      "Starting cross-validation...\n",
      "--- Fold 1 ---\n",
      "Fold 1 Validation AUC: 0.9977\n",
      "--- Fold 2 ---\n",
      "Fold 2 Validation AUC: 0.9990\n",
      "--- Fold 3 ---\n",
      "Fold 3 Validation AUC: 0.9971\n",
      "--- Fold 4 ---\n",
      "Fold 4 Validation AUC: 0.9979\n",
      "--- Fold 5 ---\n",
      "Fold 5 Validation AUC: 0.9969\n",
      "Mean Validation AUC: 0.9977 ± 0.0007\n",
      "Averaged predictions saved to 'test_predictions_tfidf_cv_rf.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "RANDOM_SEED = 3612\n",
    "\n",
    "# Step 1: Load and inspect the data\n",
    "print(\"Loading notes data...\")\n",
    "notes_df = pd.read_csv('notes.csv')\n",
    "\n",
    "# Step 2: Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "notes_df['processed_text'] = notes_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Step 3: Extract TF-IDF features\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 3))  # Expand the n-gram range\n",
    "text_features = tfidf.fit_transform(notes_df['processed_text']).toarray()\n",
    "\n",
    "# Save the extracted features\n",
    "text_features_df = pd.DataFrame(text_features, columns=tfidf.get_feature_names_out())\n",
    "text_features_df['id'] = notes_df['id']\n",
    "text_features_df.to_csv('notes_features_tfidf.csv', index=False)\n",
    "\n",
    "# Step 4: Load training and testing data\n",
    "print(\"Loading train and test data...\")\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Merge the text features with train and test data\n",
    "train_data = train_data.merge(text_features_df, on='id', how='left')\n",
    "test_data = test_data.merge(text_features_df, on='id', how='left')\n",
    "\n",
    "# Fill missing values with zero\n",
    "train_data.fillna(0, inplace=True)\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "# Step 5: Prepare features and labels\n",
    "feature_cols = [col for col in train_data.columns if col not in ['id', 'readmitted_within_30days', 'text', 'processed_text']]\n",
    "\n",
    "# Check if all feature columns are numeric\n",
    "non_numeric_cols = [col for col in feature_cols if train_data[col].dtype == 'object']\n",
    "if non_numeric_cols:\n",
    "    print(f\"Non-numeric columns detected and excluded: {non_numeric_cols}\")\n",
    "    feature_cols = [col for col in feature_cols if col not in non_numeric_cols]\n",
    "\n",
    "# Prepare training and testing data\n",
    "X = train_data[feature_cols].values\n",
    "y = train_data['readmitted_within_30days'].values\n",
    "X_test = test_data[feature_cols].values\n",
    "\n",
    "# Step 6: Cross-validation\n",
    "print(\"Starting cross-validation...\")\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "oof_preds = []\n",
    "test_preds = []\n",
    "auc_scores = []\n",
    "\n",
    "# Define Random Forest parameters\n",
    "rf_params = {\n",
    "    \"n_estimators\": 100,         # Number of trees in the forest\n",
    "    \"max_depth\": 10,             # Maximum depth of the tree\n",
    "    \"min_samples_split\": 2,      # Minimum number of samples required to split an internal node\n",
    "    \"min_samples_leaf\": 1,       # Minimum number of samples required to be at a leaf node\n",
    "    \"max_features\": 'sqrt',      # Number of features to consider when looking for the best split\n",
    "    \"random_state\": RANDOM_SEED,\n",
    "    \"n_jobs\": -1                  # Use all available cores\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"--- Fold {fold + 1} ---\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Initialize the Random Forest classifier\n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    \n",
    "    # Train the model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities for validation and test sets\n",
    "    val_preds = rf.predict_proba(X_val)[:, 1]\n",
    "    test_preds_fold = rf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store the predictions\n",
    "    oof_preds.extend(val_preds)\n",
    "    test_preds.append(test_preds_fold)\n",
    "    \n",
    "    # Calculate and store the AUC score\n",
    "    auc = roc_auc_score(y_val, val_preds)\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"Fold {fold + 1} Validation AUC: {auc:.4f}\")\n",
    "\n",
    "# Step 7: Average predictions for the test set\n",
    "final_test_preds = np.mean(test_preds, axis=0)\n",
    "\n",
    "# Step 8: Save the prediction results\n",
    "test_data['readmitted_within_30days_pred'] = final_test_preds\n",
    "averaged_predictions = (\n",
    "    test_data.groupby('id', as_index=False)['readmitted_within_30days_pred']\n",
    "    .mean()\n",
    ")\n",
    "averaged_predictions.to_csv('test_predictions_tfidf_cv_rf.csv', index=False)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the AUC scores\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "print(f\"Mean Validation AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "print(\"Averaged predictions saved to 'test_predictions_tfidf_cv_rf.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
